{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'HitLogID': 0, 'UniqID': 1, 'IsClick': 2, 'ShowTime': 3, 'PhraseID': 4, 'TargetDomainID': 5, 'PageID': 6, 'OrderID': 7, 'BannerID': 8, 'QueryLemmaH': 9, 'BannerTitleLemmaH': 10, 'DeviceType': 11, 'RegionID': 12}\n",
      "['PageID', 'OrderID', 'IsClick']\n",
      "['PageID', 'OrderID']\n",
      "[[258763, 18951888, 0], [264633, 14829991, 1], [249430, 1026618, 0], [261025, 19335144, 1]]\n"
     ]
    }
   ],
   "source": [
    "filename = \"./fixtures/net_20180312_201803114_100k\"\n",
    "features = \"PageID OrderID\".split()\n",
    "target = \"IsClick\"\n",
    "columns = features + [target]\n",
    "data = []\n",
    "with open(filename) as fd:\n",
    "    header = fd.readline()[2:].strip().split(\"\\t\")\n",
    "    col_to_index = dict((col, i) for i, col in enumerate(header))\n",
    "    column_indices = [col_to_index[col] for col in columns]\n",
    "    for line in fd:\n",
    "        splitted = line.strip().split(\"\\t\")\n",
    "        data.append([int(splitted[idx]) for idx in column_indices])\n",
    "\n",
    "print(col_to_index)        \n",
    "print(columns)\n",
    "print(features)\n",
    "print(data[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate feature map and filter rare features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique features after filtering reare features = 19135\n"
     ]
    }
   ],
   "source": [
    "# Calculate feature map via simple enumeration\n",
    "from collections import defaultdict\n",
    "\n",
    "feature_stats = defaultdict(dict)\n",
    "for example in data:\n",
    "    for fid, col in zip(example[:-1], features):\n",
    "        feature_stats[col][fid] = feature_stats[col].get(fid, 0) + 1\n",
    "\n",
    "# calculate feature map + filter rare features \n",
    "min_counts = 2\n",
    "X, y = [], []\n",
    "unk_fid = 0\n",
    "fid = 1\n",
    "feature_map = defaultdict(dict)\n",
    "for column, col_data in feature_stats.items():\n",
    "    for feature, counts in col_data.items():\n",
    "        if counts >= min_counts:\n",
    "            feature_map[column][feature] = fid\n",
    "            fid += 1\n",
    "        else:\n",
    "            feature_map[column][feature] = unk_fid\n",
    "print(\"Number of unique features after filtering reare features = {}\".format(fid))\n",
    "num_features = fid\n",
    "\n",
    "# remap features in dataset\n",
    "for rec in data:\n",
    "    X.append([feature_map[column][fid] for column, fid in zip(features, rec[:-1])])\n",
    "    y.append(rec[-1])\n",
    "\n",
    "from collections import namedtuple\n",
    "Dataset = namedtuple(\"Dataset\", \"X y\")    \n",
    "dataset = Dataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train / Test split\n",
    "import numpy as np\n",
    "\n",
    "dataset = Dataset(np.asarray(dataset.X), np.asarray(dataset.y))\n",
    "indices = np.arange(len(dataset.X))\n",
    "np.random.shuffle(indices)\n",
    "test_begin = int(0.9 * len(indices))\n",
    "train = Dataset(np.take(dataset.X, indices[:test_begin], axis=0), \n",
    "                np.take(dataset.y, indices[:test_begin], axis=0))\n",
    "test = Dataset(np.take(dataset.X, indices[test_begin:], axis=0), \n",
    "                np.take(dataset.y, indices[test_begin:], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_iter(dataset, batch_size):\n",
    "    for start in range(0, len(dataset.X), batch_size):\n",
    "        yield dataset.X[start:start + batch_size], dataset.y[start:start + batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def glorot(m):\n",
    "    size = m.weight.size() # returns a tuple\n",
    "    fan_out = size[0] # number of rows\n",
    "    fan_in = size[1]\n",
    "    if isinstance(m, nn.Linear):\n",
    "        scale = math.sqrt(2.0 / (fan_in + fan_out))\n",
    "    elif isinstance(m, nn.Embedding):\n",
    "        scale = math.sqrt(2.0 / (1.0 + fan_in * fan_out))\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    m.weight.data.uniform_(-scale, scale)\n",
    "\n",
    "    \n",
    "class FFM(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(FFM, self).__init__()\n",
    "        self.num_features = kwargs[\"num_features\"]\n",
    "        self.dim = kwargs[\"dim\"]\n",
    "        self.num_fields = kwargs[\"num_fields\"]\n",
    "        self.use_unary = kwargs[\"use_unary\"]\n",
    "        \n",
    "        # create parameters\n",
    "        self.embeddings = nn.Embedding(self.num_features, self.dim)\n",
    "        out_dim = self.dim\n",
    "        if self.use_unary:\n",
    "            self.unary = nn.Embedding(self.num_features, 1)\n",
    "            out_dim += self.num_fields\n",
    "        self.projection = nn.Linear(out_dim, 2)\n",
    "        # initialize parameters\n",
    "        glorot(self.embeddings)\n",
    "        glorot(self.projection)\n",
    "        if self.use_unary:\n",
    "            glorot(self.unary)        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        :param self:\n",
    "        :param X: B (batch size) x F (number of features)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # quadratic cross embeddings: (a+b+c)**2 - a**2 - b**2 - c**2 = 2 * (ab + bc + ac)\n",
    "        embeddings = self.embeddings(X)  # B x F x D\n",
    "        embeddings_sum = embeddings.sum(dim=1)  # B x D\n",
    "        sum_squares = torch.mul(embeddings, embeddings).sum(dim=1)  # B x D\n",
    "        quadratic = 0.5 * (torch.mul(embeddings_sum, embeddings_sum) - sum_squares)\n",
    "        if self.use_unary:\n",
    "            unary = self.unary(X)  # B x F x 1\n",
    "            unary = unary.squeeze(dim=2)  # B x F\n",
    "            out = torch.cat((quadratic, unary), dim=1)  # B x (F + D)\n",
    "        else:\n",
    "            out = quadratic\n",
    "        y = self.projection(out)\n",
    "        logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        y = logsoftmax(y)\n",
    "        return y\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA = False\n",
    "#gpus = [0]\n",
    "#torch.cuda.set_device(gpus[0])\n",
    "LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it=-1, test loss=0.6518734693527222\n",
      "it=0, train loss=409.041015625, test_loss=0.566365659236908\n",
      "it=1, train loss=375.3892517089844, test_loss=0.5507497787475586\n",
      "it=2, train loss=347.3578186035156, test_loss=0.5423299074172974\n",
      "it=3, train loss=319.36700439453125, test_loss=0.560049831867218\n",
      "it=4, train loss=299.4649963378906, test_loss=0.5908413529396057\n"
     ]
    }
   ],
   "source": [
    "features = \"PageID OrderID\".split()\n",
    "# Only factorization\n",
    "conf = {\n",
    "    \"use_unary\": False,\n",
    "    \"num_features\": num_features,\n",
    "    \"dim\": 10,\n",
    "    \"num_iter\": 5,\n",
    "    \"batch_size\": 128,\n",
    "    \"num_fields\": len(features)\n",
    "}\n",
    "\n",
    "model = FFM(**conf)\n",
    "loss_func = nn.NLLLoss()\n",
    "#optimizer = optim.SGD(model.parameters(), lr=5.0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Loss on test before learning\n",
    "test_targets = autograd.Variable(LongTensor(test.y))\n",
    "test_features = autograd.Variable(LongTensor(test.X))\n",
    "test_logprob = model.forward(test_features)\n",
    "test_loss = loss_func(test_logprob, test_targets)\n",
    "print(\"it={it}, test loss={loss}\".format(it=-1, loss=float(test_loss)))\n",
    "\n",
    "iter_loss = []\n",
    "for it in range(conf[\"num_iter\"]):\n",
    "    data_iter = batch_iter(train, batch_size=conf[\"batch_size\"])\n",
    "    batch_loss = torch.Tensor([0])\n",
    "    iter_loss.append(0)\n",
    "\n",
    "    for Xb, yb in data_iter:\n",
    "        targets = autograd.Variable(LongTensor(yb))\n",
    "        features = autograd.Variable(LongTensor(Xb))\n",
    "        model.zero_grad()\n",
    "        logprob = model.forward(features)\n",
    "        loss = loss_func(logprob, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iter_loss[-1] += loss.data\n",
    "        \n",
    "    model.zero_grad()\n",
    "    test_logprob = model.forward(test_features)\n",
    "    test_loss = loss_func(test_logprob, test_targets)\n",
    "    print(\"it={it}, train loss={loss}, test_loss={test}\".format(it=it, loss=float(iter_loss[-1]),\n",
    "                                                                test=float(test_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it=-1, test loss=0.6916554570198059\n",
      "it=0, train loss=429.96173095703125, test_loss=0.5636969208717346\n",
      "it=1, train loss=372.3305969238281, test_loss=0.535670816898346\n",
      "it=2, train loss=352.6449279785156, test_loss=0.5236057639122009\n",
      "it=3, train loss=337.5802917480469, test_loss=0.5173668265342712\n",
      "it=4, train loss=321.42633056640625, test_loss=0.5166086554527283\n",
      "it=5, train loss=303.61712646484375, test_loss=0.5264502763748169\n",
      "it=6, train loss=287.7060546875, test_loss=0.5458723306655884\n",
      "it=7, train loss=275.47357177734375, test_loss=0.5688799023628235\n",
      "it=8, train loss=265.957763671875, test_loss=0.5922778844833374\n",
      "it=9, train loss=258.05462646484375, test_loss=0.6154462695121765\n"
     ]
    }
   ],
   "source": [
    "features = \"PageID OrderID\".split()\n",
    "conf = {\n",
    "    \"use_unary\": True,\n",
    "    \"num_features\": num_features,\n",
    "    \"dim\": 10,\n",
    "    \"num_iter\": 10,\n",
    "    \"batch_size\": 128,\n",
    "    \"num_fields\": len(features)\n",
    "}\n",
    "\n",
    "model = FFM(**conf)\n",
    "loss_func = nn.NLLLoss()\n",
    "#optimizer = optim.SGD(model.parameters(), lr=5.0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "# Loss on test before learning\n",
    "test_targets = autograd.Variable(LongTensor(test.y))\n",
    "test_features = autograd.Variable(LongTensor(test.X))\n",
    "test_logprob = model.forward(test_features)\n",
    "test_loss = loss_func(test_logprob, test_targets)\n",
    "print(\"it={it}, test loss={loss}\".format(it=-1, loss=float(test_loss)))\n",
    "\n",
    "iter_loss = []\n",
    "for it in range(conf[\"num_iter\"]):\n",
    "    data_iter = batch_iter(train, batch_size=conf[\"batch_size\"])\n",
    "    batch_loss = torch.Tensor([0])\n",
    "    iter_loss.append(0)\n",
    "\n",
    "    for Xb, yb in data_iter:\n",
    "        targets = autograd.Variable(LongTensor(yb))\n",
    "        features = autograd.Variable(LongTensor(Xb))\n",
    "        model.zero_grad()\n",
    "        logprob = model.forward(features)\n",
    "        loss = loss_func(logprob, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iter_loss[-1] += loss.data\n",
    "        \n",
    "    model.zero_grad()\n",
    "    test_logprob = model.forward(test_features)\n",
    "    test_loss = loss_func(test_logprob, test_targets)\n",
    "    print(\"it={it}, train loss={loss}, test_loss={test}\".format(it=it, loss=float(iter_loss[-1]),\n",
    "                                                                test=float(test_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
